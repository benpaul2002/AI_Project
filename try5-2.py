# -*- coding: utf-8 -*-
"""try5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A8G5f9_4-7JGxuvSL4ymmlWiTBiQYMlv
"""

import pandas as pd
from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
from sklearn.model_selection import train_test_split
import torch.nn.functional as F

phi2_df = pd.read_csv('phi2_preds_dataset_large.csv')
df = phi2_df.copy()
df['is_correct'] = df['is_correct'].astype(int)

# Split with stratification to preserve class balance
train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['is_correct']  # Critical for imbalanced data
)

# Initialize model (alternatives: 'multi-qa-mpnet-base-dot-v1' or 'all-mpnet-base-v2')


### Section 1 Start
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings
sent_train_embeddings = embedding_model.encode(
    train_df['question'].tolist(),
    show_progress_bar=True,
    convert_to_tensor=True  # Optimize for GPU if available
)
sent_train_embeddings = sent_train_embeddings.cpu().numpy()
sent_test_embeddings = embedding_model.encode(test_df['question'].tolist())

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def get_models(train_embeddings):
    # Logistic Regression (good baseline)
    lr_model = LogisticRegression(
        class_weight='balanced',  # Adjusts for class imbalance
        max_iter=1000,
        random_state=42
    )
    lr_model.fit(train_embeddings, train_df['is_correct'])

    # XGBoost (often better performance)
    xgb_model = XGBClassifier(
        scale_pos_weight=len(train_df[train_df['is_correct']==0])/len(train_df[train_df['is_correct']==1]),
        eval_metric='logloss',
        use_label_encoder=False
    )
    xgb_model.fit(train_embeddings, train_df['is_correct'])

    return lr_model, xgb_model

lr_model, xgb_model = get_models(sent_train_embeddings)

from sklearn.metrics import f1_score

def get_stats(lr_model, xgb_model, test_embeddings):
    y_true = test_df['is_correct']
    y_pred = xgb_model.predict(test_embeddings)
    y_pred2 = lr_model.predict(test_embeddings)

    # For binary classification (focus on positive class)
    binary_f1 = f1_score(y_true, y_pred, average='binary')  # Default for binary
    binary_f12 = f1_score(y_true, y_pred2, average='binary')  # Default for binary

    # For class-balanced evaluation (equal weight to both classes)
    macro_f1 = f1_score(y_true, y_pred, average='macro')
    macro_f12 = f1_score(y_true, y_pred2, average='macro')

    # For imbalanced datasets (weighted by class support)
    weighted_f1 = f1_score(y_true, y_pred, average='weighted')
    weighted_f12 = f1_score(y_true, y_pred2, average='weighted')

    print(f"Binary F1 Score: {binary_f1:.2f}")
    print(f"Macro F1 Score: {macro_f1:.2f}")
    print(f"Weighted F1 Score: {weighted_f1:.2f}")

    print("\nLogistic regression:")
    print(f"Binary F1 Score: {binary_f12:.2f}")
    print(f"Macro F1 Score: {macro_f12:.2f}")
    print(f"Weighted F1 Score: {weighted_f12:.2f}")

get_stats(lr_model, xgb_model, sent_test_embeddings)

# Binary F1 Score: 0.35
# Macro F1 Score: 0.52
# Weighted F1 Score: 0.56

# Logistic regression:
# Binary F1 Score: 0.51
# Macro F1 Score: 0.55
# Weighted F1 Score: 0.57
### Section 1 End

### Section 2 Start

tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")

def generate_embeddings(texts, model, tokenizer):
    embeddings = []
    for text in texts:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding="max_length"
        )
        with torch.no_grad():
            outputs = model(**inputs)
        emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        embeddings.append(emb)
    return np.array(embeddings)

# Usage for CodeBERT:
bert_train_embeddings = generate_embeddings(train_df['question'].tolist(), model, tokenizer)
bert_test_embeddings = generate_embeddings(test_df['question'].tolist(), model, tokenizer)

lr_model, xgb_model = get_models(bert_train_embeddings)
get_stats(lr_model, xgb_model, bert_test_embeddings)

# """Binary F1 Score: 0.42 \\
# Macro F1 Score: 0.57 \\
# Weighted F1 Score: 0.60 \\

# Logistic regression: \\
# Binary F1 Score: 0.41 \\
# Macro F1 Score: 0.51 \\
# Weighted F1 Score: 0.53
# """
# ### Section 2 End

# ### Section 3 Start

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class TransformerClassifier(nn.Module):
    def __init__(self, embedding_dim):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 2)
        )

    def forward(self, x):
        return self.layers(x)

# Convert embeddings to PyTorch tensors
train_embeddings_tensor = torch.tensor(sent_train_embeddings, dtype=torch.float32)
train_labels_tensor = torch.tensor(train_df['is_correct'].values, dtype=torch.long)

# Create DataLoader
train_dataset = TensorDataset(train_embeddings_tensor, train_labels_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Initialize model
model = TransformerClassifier(sent_train_embeddings.shape[1])
optimizer = optim.Adam(model.parameters(), lr=0.001)
# criterion = nn.CrossEntropyLoss(weight=t  orch.tensor([1.0, 2.0]))  # Adjust class weights

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    def forward(self, logits, targets):
        ce = F.cross_entropy(logits, targets, reduction='none')
        pt = torch.exp(-ce)
        focal = self.alpha * (1-pt)**self.gamma * ce
        return focal.mean()

criterion = FocalLoss(alpha=0.25, gamma=2)

# Training loop
model.train()
for epoch in range(10):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

from sklearn.metrics import classification_report

# Convert test data to tensors
test_embeddings_tensor = torch.tensor(sent_test_embeddings, dtype=torch.float32)
test_labels_tensor = torch.tensor(test_df['is_correct'].values, dtype=torch.long)

# Prediction function
def predict(model, embeddings_tensor):
    model.eval()
    with torch.no_grad():
        outputs = model(embeddings_tensor)
        _, preds = torch.max(outputs, 1)
    return preds.numpy()

# Get predictions
test_preds = predict(model, test_embeddings_tensor)

# Generate report
print(classification_report(
    test_df['is_correct'],
    test_preds,
    target_names=['Incorrect', 'Correct']
))

#              precision    recall  f1-score   support

#    Incorrect       0.64      0.74      0.68       122
#      Correct       0.46      0.35      0.39        78

#     accuracy                           0.58       200
#    macro avg       0.55      0.54      0.54       200
# weighted avg       0.57      0.58      0.57       200

# ### Section 3 End

### Section 4 Start

import torch
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import classification_report
from sklearn.utils.class_weight import compute_class_weight

# Initialize tokenizer (same as original model)
# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
# model = DistilBertForSequenceClassification.from_pretrained(
#     'distilbert-base-uncased',
#     num_labels=2,
#     problem_type="single_label_classification"
# )

tokenizer = AutoTokenizer.from_pretrained("roberta-base")
model = AutoModelForSequenceClassification.from_pretrained(
    "roberta-base", 
    num_labels=2
)

# Add numerical tokens explicitly (critical for math problems)
new_tokens = [str(i) for i in range(1000)] + ["+", "-", "*", "/", "^", "="]
tokenizer.add_tokens(new_tokens)
model.resize_token_embeddings(len(tokenizer))

model.config.id2label = {0: "incorrect", 1: "correct"}

# Tokenization function with padding/truncation
def tokenize_data(texts, labels, max_length=128):
    encodings = tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt'
    )
    return TensorDataset(
        encodings['input_ids'],
        encodings['attention_mask'],
        torch.tensor(labels)
    )

# Convert DataFrame to tensors
train_dataset = tokenize_data(train_df['question'].tolist(), train_df['is_correct'].tolist())
test_dataset = tokenize_data(test_df['question'].tolist(), test_df['is_correct'].tolist())

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

y_train = train_df['is_correct'].tolist()
classes = np.unique(y_train)  # Dynamically get classes from data
class_weights = compute_class_weight(
    'balanced',
    classes=classes,
    y=y_train
)
weights = torch.tensor(class_weights, dtype=torch.float)

# Training setup
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
# loss_fn = torch.nn.CrossEntropyLoss(weight=weights)

# Add this custom focal loss class before your training loop
class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = torch.nn.functional.cross_entropy(
            inputs,
            targets,
            reduction='none',
            weight=weights.to(inputs.device)  # Use your precomputed class weights
        )
        pt = torch.exp(-ce_loss)
        focal_loss = (self.alpha * (1-pt)**self.gamma * ce_loss)

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

# 1. Adjust Focal Loss parameters for your class distribution (122 vs 78)
loss_fn = FocalLoss(alpha=0.5, gamma=2.5)  # Increased alpha for class 1 focus

# 2. Add learning rate warmup (critical for transformer fine-tuning)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
total_steps = len(train_loader) * 5  # Increased to 5 epochs
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=2e-5,
    total_steps=total_steps,
    pct_start=0.1  # 10% of steps for warmup
)

# 3. Modify training loop to include scheduler
model.train()
for epoch in range(5):  # Increased epochs
    total_loss = 0
    for batch_idx, batch in enumerate(train_loader):
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs.logits, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
        optimizer.step()
        scheduler.step()  # Update learning rate
        total_loss += loss.item()
    print(f"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}")

# Remove the redundant evaluation at the end - keep only threshold-optimized report


# Add after model training but before final evaluation
from sklearn.metrics import precision_recall_curve

# Get predicted probabilities
test_probs = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, _ = batch
        outputs = model(input_ids, attention_mask=attention_mask)
        test_probs.extend(torch.nn.functional.softmax(outputs.logits, dim=1)[:,1].tolist())

# Find optimal threshold that maximizes F1-score
precisions, recalls, thresholds = precision_recall_curve(test_df['is_correct'], test_probs)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
optimal_threshold = thresholds[np.argmax(f1_scores)]

# Re-evaluate with optimized threshold
final_preds = [1 if prob > optimal_threshold else 0 for prob in test_probs]
print(f"Optimal Threshold: {optimal_threshold:.3f}")
print(classification_report(test_df['is_correct'], final_preds))

# save_path = "./my_finetuned_model"
# model.save_pretrained(save_path)
# tokenizer.save_pretrained(save_path)

# !zip -r my_finetuned_model.zip my_finetuned_model

# from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast

# loaded_model = DistilBertForSequenceClassification.from_pretrained(save_path)
# loaded_tokenizer = DistilBertTokenizerFast.from_pretrained(save_path)

# # Usage example
# text = "Your new question here"
# inputs = loaded_tokenizer(text, return_tensors="pt")
# outputs = loaded_model(**inputs)


#               precision    recall  f1-score   support

#            0       0.77      0.40      0.53       122
#            1       0.46      0.81      0.59        78

#     accuracy                           0.56       200
#    macro avg       0.61      0.60      0.56       200
# weighted avg       0.65      0.56      0.55       200

### Section 4 End