{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "493cd09ea4574d199a670b177dbd3a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eab44f4955e24af982adadd52a578d21",
              "IPY_MODEL_c7da4a57d3634e7f85a4f49e6a38efd9",
              "IPY_MODEL_45eea6b803a54fac8f2a8a2eab809310"
            ],
            "layout": "IPY_MODEL_b46f41474023400186983a6b865eaac4"
          }
        },
        "eab44f4955e24af982adadd52a578d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d33a558a2c20491abe28bd45616fa16f",
            "placeholder": "​",
            "style": "IPY_MODEL_3691b448052d486d89fd0b12aa76d9c4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c7da4a57d3634e7f85a4f49e6a38efd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5ce879526ea434dacb4513f57d06f48",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5de0421fb8f439b8e157bb3667aa81f",
            "value": 2
          }
        },
        "45eea6b803a54fac8f2a8a2eab809310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff5d646401544ee09f557c9cba168074",
            "placeholder": "​",
            "style": "IPY_MODEL_a4a6bef1a0d7487b8b594dd06aa85cf6",
            "value": " 2/2 [00:25&lt;00:00, 10.86s/it]"
          }
        },
        "b46f41474023400186983a6b865eaac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d33a558a2c20491abe28bd45616fa16f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3691b448052d486d89fd0b12aa76d9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5ce879526ea434dacb4513f57d06f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5de0421fb8f439b8e157bb3667aa81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff5d646401544ee09f557c9cba168074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4a6bef1a0d7487b8b594dd06aa85cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0baf9ba3ddb8495f916e49e38c6628ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14d2c5b05be6445289018e982d35e41f",
              "IPY_MODEL_b092edf5c3a64c83a831d3a6035d5a14",
              "IPY_MODEL_d7b6207ef8f844fb8038f400429a04c3"
            ],
            "layout": "IPY_MODEL_f0947379bc8d427e8d6fcf50d6302d9f"
          }
        },
        "14d2c5b05be6445289018e982d35e41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c937b0603f744370b44acc474ef26264",
            "placeholder": "​",
            "style": "IPY_MODEL_31e0984faf91453493f9443122ec838f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b092edf5c3a64c83a831d3a6035d5a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f520526dc9d4ccbb2be8259d69f344b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdf8c5b8140b4f7ca5de7573b0082ea6",
            "value": 2
          }
        },
        "d7b6207ef8f844fb8038f400429a04c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30bc9c7b1e98402eada4c52113916e78",
            "placeholder": "​",
            "style": "IPY_MODEL_d08ac3d93918468481509d81a0527e59",
            "value": " 2/2 [01:21&lt;00:00, 38.09s/it]"
          }
        },
        "f0947379bc8d427e8d6fcf50d6302d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c937b0603f744370b44acc474ef26264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e0984faf91453493f9443122ec838f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f520526dc9d4ccbb2be8259d69f344b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf8c5b8140b4f7ca5de7573b0082ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30bc9c7b1e98402eada4c52113916e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d08ac3d93918468481509d81a0527e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install datasets\n",
        "!pip install textstat\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaXlozKfgrL0",
        "outputId": "d0221bed-4548-476c-b823-2f6d14f1edb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=6008021 sha256=12e98eb576a34120e7cec95c8a89e47a3676917fd7f47ee280cf684cc67687e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.5\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MgAp3mNWgKM9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import nltk\n",
        "import textstat\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-apNuUvgcA1",
        "outputId": "a77e7ca6-8122-4efb-e38a-53aa5dbc1303"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Q_Learner():\n",
        "    def __init__(self, models, learning_rate=0.1, discount_factor=0.9, epsilon=0.1, load_q_table=False):\n",
        "        self.models = models\n",
        "        # With 3 complexity levels and 3 length levels\n",
        "        complexity_levels = [\"simple\", \"moderate\", \"complex\"]\n",
        "        length_levels = [\"short\", \"medium\", \"long\"]\n",
        "        self.num_models = len(models)\n",
        "\n",
        "        # Initialize Q-table\n",
        "        if load_q_table:\n",
        "            with open('q_table.pkl', 'rb') as f:\n",
        "                self.q_table = pickle.load(f)\n",
        "        else:\n",
        "            q_table = {}\n",
        "            for complexity in complexity_levels:\n",
        "                for length in length_levels:\n",
        "                    q_table[(complexity, length)] = [0] * self.num_models\n",
        "        self.q_table = q_table\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "\n",
        "        self.stats = {\"cheap_model_uses\": 0, \"expensive_model_uses\": 0, \"rewards\": []}\n",
        "\n",
        "    def choose_model(self, state):\n",
        "        # Epsilon-greedy policy for exploration vs exploitation\n",
        "        if random.random() < self.epsilon:\n",
        "            # Exploration: randomly choose a model\n",
        "            model_index = random.randint(0, len(self.models) - 1)\n",
        "        else:\n",
        "            # Exploitation: choose model with highest Q-value\n",
        "            model_index = np.argmax(self.q_table[state])\n",
        "\n",
        "        # Track usage statistics\n",
        "        if model_index == 0:\n",
        "            self.stats[\"cheap_model_uses\"] += 1\n",
        "        else:\n",
        "            self.stats[\"expensive_model_uses\"] += 1\n",
        "\n",
        "        return model_index, self.models[model_index]\n",
        "\n",
        "    def decay_epsilon(self, decay_rate=0.995):\n",
        "        self.epsilon *= decay_rate\n",
        "        self.epsilon = max(self.epsilon, 0.01)  # Minimum exploration rate\n",
        "\n",
        "    def calculate_reward(self, model_index, is_correct):\n",
        "        model_cost = self.models[model_index]['cost']\n",
        "        # Base reward: 1 for correct, 0 for incorrect\n",
        "        performance_score = 1 if is_correct else 0\n",
        "        # Higher cost penalty for incorrect answers makes sense\n",
        "        cost_factor = 0.05 if is_correct else 0.1\n",
        "        reward = performance_score - (cost_factor * model_cost)\n",
        "        self.stats[\"rewards\"].append(reward)\n",
        "        return reward\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        # Standard Q-learning update formula\n",
        "        current_q = self.q_table[state][action]\n",
        "        best_next_q = max(self.q_table[next_state])\n",
        "\n",
        "        new_q = current_q + self.learning_rate * (\n",
        "            reward + self.discount_factor * best_next_q - current_q\n",
        "        )\n",
        "\n",
        "        self.q_table[state][action] = new_q\n",
        "\n",
        "    def calculate_complexity(self, text):\n",
        "        # Option 1: Readability metrics\n",
        "        readability_score = textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "        # Option 2: Vocabulary diversity\n",
        "        unique_words = len(set(text.lower().split()))\n",
        "        total_words = len(text.split())\n",
        "        lexical_diversity = unique_words / total_words\n",
        "\n",
        "        # Option 3: Sentence complexity\n",
        "        avg_sentence_length = sum(len(s.split()) for s in nltk.sent_tokenize(text)) / len(nltk.sent_tokenize(text))\n",
        "\n",
        "        # Combined score (example)\n",
        "        return readability_score + (lexical_diversity * 50) + (avg_sentence_length * 2)\n",
        "\n",
        "    def get_state(self, text):\n",
        "        # Length discretization\n",
        "        num_words = len(text.split())\n",
        "        if num_words < 50:\n",
        "            length = \"short\"\n",
        "        elif num_words < 200:\n",
        "            length = \"medium\"\n",
        "        else:\n",
        "            length = \"long\"\n",
        "\n",
        "        # Complexity discretization (example using readability)\n",
        "        complexity_score = self.calculate_complexity(text)\n",
        "        if complexity_score < 30:\n",
        "            complexity = \"simple\"\n",
        "        elif complexity_score < 70:\n",
        "            complexity = \"moderate\"\n",
        "        else:\n",
        "            complexity = \"complex\"\n",
        "\n",
        "        return (complexity, length)  # State tuple\n",
        "\n",
        "    def save_q_table(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.q_table, f)\n",
        "\n",
        "    def load_q_table(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.q_table = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "wWkyj7uTgh9Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import textstat\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DQN_Learner():\n",
        "    def __init__(self, models, learning_rate=0.001, discount_factor=0.9, epsilon=0.1,\n",
        "                 epsilon_decay=0.995, epsilon_min=0.01, load_model=False):\n",
        "        self.models = models\n",
        "        self.num_models = len(models)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        # State space: complexity (3 levels) and length (3 levels) one-hot encoded\n",
        "        self.state_size = 6  # 3 complexity + 3 length\n",
        "        self.action_size = self.num_models\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = DQN(self.state_size, self.action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        if load_model:\n",
        "            self.model.load_state_dict(torch.load('dqn_model.pth'))\n",
        "\n",
        "        self.stats = {\"cheap_model_uses\": 0, \"expensive_model_uses\": 0, \"rewards\": []}\n",
        "\n",
        "    def one_hot_state(self, state):\n",
        "        complexity_levels = [\"simple\", \"moderate\", \"complex\"]\n",
        "        length_levels = [\"short\", \"medium\", \"long\"]\n",
        "        state_vec = np.zeros(self.state_size)\n",
        "        state_vec[complexity_levels.index(state[0])] = 1\n",
        "        state_vec[3 + length_levels.index(state[1])] = 1\n",
        "        return torch.tensor(state_vec, dtype=torch.float32).to(self.device)\n",
        "\n",
        "    def choose_model(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            model_index = random.randint(0, self.num_models - 1)\n",
        "        else:\n",
        "            state_tensor = self.one_hot_state(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.model(state_tensor)\n",
        "            model_index = torch.argmax(q_values).item()\n",
        "\n",
        "        if model_index == 0:\n",
        "            self.stats[\"cheap_model_uses\"] += 1\n",
        "        else:\n",
        "            self.stats[\"expensive_model_uses\"] += 1\n",
        "\n",
        "        return model_index, self.models[model_index]\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "    def calculate_reward(self, model_index, is_correct):\n",
        "        model_cost = self.models[model_index]['cost']\n",
        "        performance_score = 1 if is_correct else 0\n",
        "        cost_factor = 0.05 if is_correct else 0.1\n",
        "        reward = performance_score - (cost_factor * model_cost)\n",
        "        self.stats[\"rewards\"].append(reward)\n",
        "        return reward\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done=False):\n",
        "        state_tensor = self.one_hot_state(state).unsqueeze(0)\n",
        "        next_state_tensor = self.one_hot_state(next_state).unsqueeze(0)\n",
        "\n",
        "        self.model.train()\n",
        "        q_values = self.model(state_tensor)\n",
        "        q_value = q_values[0, action]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.model(next_state_tensor)\n",
        "            max_next_q_value = torch.max(next_q_values)\n",
        "            target = reward + (self.discount_factor * max_next_q_value * (1 - int(done)))\n",
        "\n",
        "        loss = self.criterion(q_value, target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def calculate_complexity(self, text):\n",
        "        readability_score = textstat.flesch_kincaid_grade(text)\n",
        "        unique_words = len(set(text.lower().split()))\n",
        "        total_words = len(text.split())\n",
        "        lexical_diversity = unique_words / total_words\n",
        "        avg_sentence_length = sum(len(s.split()) for s in nltk.sent_tokenize(text)) / len(nltk.sent_tokenize(text))\n",
        "        return readability_score + (lexical_diversity * 50) + (avg_sentence_length * 2)\n",
        "\n",
        "    def get_state(self, text):\n",
        "        num_words = len(text.split())\n",
        "        if num_words < 50:\n",
        "            length = \"short\"\n",
        "        elif num_words < 200:\n",
        "            length = \"medium\"\n",
        "        else:\n",
        "            length = \"long\"\n",
        "\n",
        "        complexity_score = self.calculate_complexity(text)\n",
        "        if complexity_score < 30:\n",
        "            complexity = \"simple\"\n",
        "        elif complexity_score < 70:\n",
        "            complexity = \"moderate\"\n",
        "        else:\n",
        "            complexity = \"complex\"\n",
        "\n",
        "        return (complexity, length)\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        torch.save(self.model.state_dict(), filename)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        self.model.load_state_dict(torch.load(filename))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99M31wf-M8sJ",
        "outputId": "43d5d123-bf00-41bd-9907-7540cb02b74c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class PPO_Agent():\n",
        "    def __init__(self, models, state_dim=6, learning_rate=0.001, gamma=0.99,\n",
        "                 clip_epsilon=0.2, ppo_epochs=4, batch_size=32):\n",
        "        self.models = models\n",
        "        self.num_models = len(models)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize networks\n",
        "        self.policy_network = PolicyNetwork(state_dim, self.num_models).to(self.device)\n",
        "        self.value_network = ValueNetwork(state_dim).to(self.device)\n",
        "\n",
        "        # Initialize optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
        "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=learning_rate)\n",
        "\n",
        "        # PPO parameters\n",
        "        self.gamma = gamma\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.ppo_epochs = ppo_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Memory for experience collection\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.next_states = []\n",
        "        self.action_probs = []\n",
        "        self.dones = []\n",
        "\n",
        "        # Statistics\n",
        "        self.stats = {\"cheap_model_uses\": 0, \"expensive_model_uses\": 0, \"rewards\": []}\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        \"\"\"Convert the state tuple to a tensor representation\"\"\"\n",
        "        # Convert categorical variables to one-hot encoding\n",
        "        complexity_map = {\"simple\": 0, \"moderate\": 1, \"complex\": 2}\n",
        "        length_map = {\"short\": 0, \"medium\": 1, \"long\": 2}\n",
        "\n",
        "        complexity, length = state\n",
        "\n",
        "        # One-hot encode complexity (3 values)\n",
        "        complexity_one_hot = [0, 0, 0]\n",
        "        complexity_one_hot[complexity_map[complexity]] = 1\n",
        "\n",
        "        # One-hot encode length (3 values)\n",
        "        length_one_hot = [0, 0, 0]\n",
        "        length_one_hot[length_map[length]] = 1\n",
        "\n",
        "        # Combine into one vector\n",
        "        state_vector = complexity_one_hot + length_one_hot\n",
        "\n",
        "        return torch.FloatTensor(state_vector).to(self.device)\n",
        "\n",
        "    def choose_model(self, state):\n",
        "        \"\"\"Choose a model based on the current policy\"\"\"\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_probs = self.policy_network(state_tensor)\n",
        "\n",
        "        # Convert to numpy for sampling\n",
        "        action_probs_np = action_probs.cpu().numpy()\n",
        "\n",
        "        # Sample action from the probability distribution\n",
        "        action = np.random.choice(self.num_models, p=action_probs_np)\n",
        "\n",
        "        # Track usage statistics\n",
        "        if action == 0:\n",
        "            self.stats[\"cheap_model_uses\"] += 1\n",
        "        else:\n",
        "            self.stats[\"expensive_model_uses\"] += 1\n",
        "\n",
        "        # Store the probability of the selected action\n",
        "        action_prob = action_probs[action].item()\n",
        "\n",
        "        return action, self.models[action], action_prob\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, action_prob, done=False):\n",
        "        \"\"\"Store experience in memory\"\"\"\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.next_states.append(next_state)\n",
        "        self.action_probs.append(action_prob)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def compute_returns(self):\n",
        "        \"\"\"Compute returns and advantages for all stored rewards\"\"\"\n",
        "        returns = []\n",
        "        advantages = []\n",
        "\n",
        "        # Convert states to tensors for value estimation\n",
        "        states_tensor = torch.stack([self.state_to_tensor(s) for s in self.states])\n",
        "        next_states_tensor = torch.stack([self.state_to_tensor(s) for s in self.next_states])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            values = self.value_network(states_tensor).squeeze()\n",
        "            next_values = self.value_network(next_states_tensor).squeeze()\n",
        "\n",
        "        # Convert to numpy\n",
        "        values = values.cpu().numpy()\n",
        "        next_values = next_values.cpu().numpy()\n",
        "\n",
        "        # Calculate returns and advantages\n",
        "        for i in reversed(range(len(self.rewards))):\n",
        "            # If this is the last step or if the episode is done\n",
        "            if i == len(self.rewards) - 1 or self.dones[i]:\n",
        "                next_return = 0\n",
        "            else:\n",
        "                next_return = returns[0]\n",
        "\n",
        "            # Calculate return (discounted reward)\n",
        "            current_return = self.rewards[i] + self.gamma * next_return\n",
        "            returns.insert(0, current_return)\n",
        "\n",
        "            # Calculate advantage\n",
        "            if self.dones[i]:\n",
        "                advantage = current_return - values[i]\n",
        "            else:\n",
        "                advantage = self.rewards[i] + self.gamma * next_values[i] - values[i]\n",
        "\n",
        "            advantages.insert(0, advantage)\n",
        "\n",
        "        return torch.FloatTensor(returns).to(self.device), torch.FloatTensor(advantages).to(self.device)\n",
        "\n",
        "    def update_policy(self):\n",
        "        \"\"\"Update policy and value networks using PPO\"\"\"\n",
        "        # If there are no experiences to learn from, return\n",
        "        if len(self.states) == 0:\n",
        "            return\n",
        "\n",
        "        # Compute returns and advantages\n",
        "        returns, advantages = self.compute_returns()\n",
        "\n",
        "        # Convert states and actions to tensors\n",
        "        states_tensor = torch.stack([self.state_to_tensor(s) for s in self.states])\n",
        "        actions = torch.LongTensor(self.actions).to(self.device)\n",
        "        old_action_probs = torch.FloatTensor(self.action_probs).to(self.device)\n",
        "\n",
        "        # Normalize advantages\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # PPO update for multiple epochs\n",
        "        for _ in range(self.ppo_epochs):\n",
        "            # Create random indices\n",
        "            indices = torch.randperm(len(self.states))\n",
        "\n",
        "            # Create mini-batches\n",
        "            for start_idx in range(0, len(self.states), self.batch_size):\n",
        "                # Get mini-batch indices\n",
        "                idx = indices[start_idx:start_idx + self.batch_size]\n",
        "\n",
        "                # Get mini-batch data\n",
        "                mb_states = states_tensor[idx]\n",
        "                mb_actions = actions[idx]\n",
        "                mb_old_action_probs = old_action_probs[idx]\n",
        "                mb_returns = returns[idx]\n",
        "                mb_advantages = advantages[idx]\n",
        "\n",
        "                # Forward pass\n",
        "                action_probs = self.policy_network(mb_states)\n",
        "                values = self.value_network(mb_states).squeeze()\n",
        "\n",
        "                # Get probabilities of the actions we actually took\n",
        "                actions_one_hot = F.one_hot(mb_actions, num_classes=self.num_models).float()\n",
        "                current_action_probs = torch.sum(action_probs * actions_one_hot, dim=1)\n",
        "\n",
        "                # Compute ratio\n",
        "                ratio = current_action_probs / mb_old_action_probs\n",
        "\n",
        "                # Compute surrogate losses\n",
        "                surrogate1 = ratio * mb_advantages\n",
        "                surrogate2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * mb_advantages\n",
        "\n",
        "                # Compute policy loss, value loss, and entropy\n",
        "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "                value_loss = F.mse_loss(values, mb_returns)\n",
        "                entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-10), dim=1).mean()\n",
        "\n",
        "                # Compute total loss (policy + value + entropy bonus)\n",
        "                total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                self.value_optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.policy_optimizer.step()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "        # Clear memory after updating\n",
        "        self.clear_memory()\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear stored experiences\"\"\"\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.next_states = []\n",
        "        self.action_probs = []\n",
        "        self.dones = []\n",
        "\n",
        "    def calculate_reward(self, model_index, is_correct):\n",
        "        \"\"\"Calculate reward based on model performance and cost\"\"\"\n",
        "        model_cost = self.models[model_index]['cost']\n",
        "        # Base reward: 1 for correct, 0 for incorrect\n",
        "        performance_score = 1 if is_correct else 0\n",
        "        # Higher cost penalty for incorrect answers\n",
        "        cost_factor = 0.05 if is_correct else 0.1\n",
        "        reward = performance_score - (cost_factor * model_cost)\n",
        "        self.stats[\"rewards\"].append(reward)\n",
        "        return reward\n",
        "\n",
        "    def get_state(self, text):\n",
        "        \"\"\"Analyze text to determine state features\"\"\"\n",
        "        # Length discretization\n",
        "        num_words = len(text.split())\n",
        "        if num_words < 50:\n",
        "            length = \"short\"\n",
        "        elif num_words < 200:\n",
        "            length = \"medium\"\n",
        "        else:\n",
        "            length = \"long\"\n",
        "\n",
        "        # Complexity discretization using readability\n",
        "        complexity_score = self.calculate_complexity(text)\n",
        "        if complexity_score < 30:\n",
        "            complexity = \"simple\"\n",
        "        elif complexity_score < 70:\n",
        "            complexity = \"moderate\"\n",
        "        else:\n",
        "            complexity = \"complex\"\n",
        "\n",
        "        return (complexity, length)\n",
        "\n",
        "    def calculate_complexity(self, text):\n",
        "        \"\"\"Calculate text complexity using various metrics\"\"\"\n",
        "        # Option 1: Readability metrics\n",
        "        readability_score = textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "        # Option 2: Vocabulary diversity\n",
        "        unique_words = len(set(text.lower().split()))\n",
        "        total_words = len(text.split())\n",
        "        lexical_diversity = unique_words / total_words if total_words > 0 else 0\n",
        "\n",
        "        # Option 3: Sentence complexity\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0\n",
        "\n",
        "        # Combined score\n",
        "        return readability_score + (lexical_diversity * 50) + (avg_sentence_length * 2)\n",
        "\n",
        "    def save_model(self, policy_path, value_path):\n",
        "        \"\"\"Save the policy and value networks\"\"\"\n",
        "        torch.save(self.policy_network.state_dict(), policy_path)\n",
        "        torch.save(self.value_network.state_dict(), value_path)\n",
        "\n",
        "    def load_model(self, policy_path, value_path):\n",
        "        \"\"\"Load the policy and value networks\"\"\"\n",
        "        self.policy_network.load_state_dict(torch.load(policy_path))\n",
        "        self.value_network.load_state_dict(torch.load(value_path))\n"
      ],
      "metadata": {
        "id": "VDnaM1ZFQQrd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_name):\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,  # Match your input dtype\n",
        "        bnb_4bit_quant_type=\"nf4\",  # Add quantization type\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    if model_name == \"wizardmath\":\n",
        "        wizardmath_tokenizer = AutoTokenizer.from_pretrained(\"WizardLM/WizardMath-7B-V1.1\")\n",
        "        wizardmath_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"WizardLM/WizardMath-7B-V1.1\",\n",
        "            quantization_config=quantization_config,\n",
        "            device_map={\"\": 0},\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        return {\n",
        "            'model': wizardmath_model,\n",
        "            'model_name': \"wizardmath\",\n",
        "            'tokenizer': wizardmath_tokenizer,\n",
        "            'cost': 10\n",
        "        }\n",
        "    elif model_name == \"phi2\":\n",
        "        phi2_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
        "        phi2_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/phi-2\",\n",
        "            quantization_config=quantization_config,\n",
        "            device_map={\"\": 0},\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        return {\n",
        "            'model': phi2_model,\n",
        "            'model_name': \"phi2\",\n",
        "            'tokenizer': phi2_tokenizer,\n",
        "            'cost': 5  # Lower cost since it's a smaller model\n",
        "        }"
      ],
      "metadata": {
        "id": "4UP9HWs9gSYG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset():\n",
        "    train_dataset = load_dataset(\"openai/gsm8k\", \"main\", split='train')\n",
        "    test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split='test')\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "3V7N0kungTts"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(answer_text):\n",
        "    # The final answer in GSM8K follows the '####' pattern\n",
        "    match = re.search(r'####\\s*(-?\\d+)', answer_text)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return None"
      ],
      "metadata": {
        "id": "Q3fEPvDSgUxG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_problem(problem, model_index, models):\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "Follow these instructions:\n",
        "1. Work through the problem step by step\n",
        "2. Calculate the numerical answer\n",
        "3. On the last line, write ONLY: #### <numerical answer>. Do not add any units like \"kg\" or \"m\", or any currency symbols like \"$\".\n",
        "4. Do not write anything after the final answer\n",
        "\n",
        "-------------------\n",
        "EXAMPLE FORMAT:\n",
        "Step 1: [explanation]\n",
        "Step 2: [explanation]\n",
        "Final calculation: [calculation]\n",
        "#### [numerical answer]\n",
        "-------------------\n",
        "\n",
        "NOW SOLVE THE PROBLEM CORRECTLY: {problem['question']}\n",
        "\"\"\"\n",
        "    model_obj = models[model_index]['model']\n",
        "    tokenizer = models[model_index].get('tokenizer', None)\n",
        "    if tokenizer:\n",
        "        tokenizer = models[model_index]['tokenizer']\n",
        "\n",
        "    # if models[model_index]['model_name'] == \"wizardmath\":\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_obj.device)\n",
        "    outputs = model_obj.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        # pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    prompt_end = full_output.find(f\"NOW SOLVE THE PROBLEM CORRECTLY: {problem['question']}\")\n",
        "    if prompt_end != -1:\n",
        "        # Move past the question to get to the solution\n",
        "        prompt_end = prompt_end + len(f\"NOW SOLVE THE PROBLEM CORRECTLY: {problem['question']}\")\n",
        "        model_response = full_output[prompt_end:].strip()\n",
        "    else:\n",
        "        # Fallback if we can't find the exact prompt ending\n",
        "        model_response = full_output\n",
        "\n",
        "    # Now extract the numeric answer using a more reliable approach\n",
        "    import re\n",
        "\n",
        "    # Check for #### pattern first (Phi-2 style)\n",
        "    hash_match = re.search(r'####\\s*([\\$]?\\s*\\d+(?:\\.\\d+)?)', model_response)\n",
        "    if hash_match:\n",
        "        # Extract just the number, removing any currency symbols\n",
        "        answer_text = hash_match.group(1)\n",
        "        numeric_match = re.search(r'(\\d+(?:\\.\\d+)?)', answer_text)\n",
        "        if numeric_match:\n",
        "            numeric_answer = numeric_match.group(1)\n",
        "            return f\"{prompt}\\n\\n{model_response.split('####')[0].strip()}\\n#### {numeric_answer}\"\n",
        "\n",
        "    # Check for explicit \"answer is\" pattern (WizardMath style)\n",
        "    answer_match = re.search(r'(?:final answer|the answer is)[^0-9]*?([\\$]?\\s*\\d+(?:\\.\\d+)?)',\n",
        "                            model_response.lower())\n",
        "    if answer_match:\n",
        "        answer_text = answer_match.group(1)\n",
        "        numeric_match = re.search(r'(\\d+(?:\\.\\d+)?)', answer_text)\n",
        "        if numeric_match:\n",
        "            numeric_answer = numeric_match.group(1)\n",
        "            # Find where this answer occurs in the text to split it there\n",
        "            answer_position = model_response.lower().find(answer_match.group(0))\n",
        "            if answer_position != -1:\n",
        "                return f\"{prompt}\\n\\n{model_response[:answer_position].strip()}\\n#### {numeric_answer}\"\n",
        "\n",
        "    # If all else fails, look for numbers in the last few lines\n",
        "    lines = model_response.split('\\n')\n",
        "    for i in range(len(lines)-1, max(0, len(lines)-5), -1):\n",
        "        line = lines[i]\n",
        "        # Skip lines that are clearly not the answer\n",
        "        if len(line.strip()) < 1 or any(word in line.lower() for word in [\"step\", \"explanation\"]):\n",
        "            continue\n",
        "\n",
        "        numeric_match = re.search(r'(\\d+(?:\\.\\d+)?)', line)\n",
        "        if numeric_match:\n",
        "            numeric_answer = numeric_match.group(1)\n",
        "            return f\"{prompt}\\n\\n{model_response.split(line)[0].strip()}\\n#### {numeric_answer}\"\n",
        "\n",
        "    # If we couldn't extract an answer, return the unmodified output\n",
        "    return full_output\n"
      ],
      "metadata": {
        "id": "AYlqij6rgY5s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_q_learner(dataset, models):\n",
        "    q_learner = Q_Learner(models)\n",
        "\n",
        "    dataset = gsm8k_dataset['train']\n",
        "    for i in tqdm(range(len(dataset)), desc=\"Training Q-learner\"):\n",
        "        current_problem = dataset[i]\n",
        "        next_problem = dataset[i+1]\n",
        "\n",
        "        current_state = q_learner.get_state(current_problem[\"question\"])\n",
        "        next_state = q_learner.get_state(next_problem[\"question\"])\n",
        "\n",
        "        model_index, model = q_learner.choose_model(current_state)\n",
        "\n",
        "        # Process the current problem\n",
        "        model_output = process_problem(current_problem, model_index, models)\n",
        "\n",
        "        # print(f\"\\nProblem {i}: {current_problem['question']}\")\n",
        "        print(f\"\\nChosen model: {model_index} ({'cheap' if model_index == 0 else 'expensive'})\")\n",
        "        # print(f\"\\nModel output: {model_output}\")\n",
        "\n",
        "        # Extract answers and check correctness\n",
        "        predicted_answer = extract_answer(model_output)\n",
        "        print(f\"Predicted answer: {predicted_answer}\")\n",
        "        true_answer = extract_answer(current_problem[\"answer\"])\n",
        "        print(f\"True answer: {true_answer}\")\n",
        "        is_correct = (predicted_answer == true_answer) if predicted_answer and true_answer else False\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = q_learner.calculate_reward(model_index, is_correct)\n",
        "\n",
        "        # Update Q-values\n",
        "        q_learner.update_q_value(current_state, model_index, reward, next_state)\n",
        "\n",
        "        # Decay epsilon after each problem\n",
        "        q_learner.decay_epsilon()\n",
        "\n",
        "    # Handle the last problem separately (terminal state)\n",
        "    last_problem = dataset[-1]\n",
        "    last_state = q_learner.get_state(last_problem[\"question\"])\n",
        "    model_index, model = q_learner.choose_model(last_state)\n",
        "\n",
        "    # Process the last problem\n",
        "    model_output = process_problem(last_problem, model_index, models)\n",
        "\n",
        "    predicted_answer = extract_answer(model_output)\n",
        "    true_answer = extract_answer(last_problem[\"answer\"])\n",
        "    is_correct = (predicted_answer == true_answer) if predicted_answer and true_answer else False\n",
        "\n",
        "    # For terminal state, just update with immediate reward\n",
        "    terminal_reward = q_learner.calculate_reward(model_index, is_correct)\n",
        "    current_q = q_learner.q_table[last_state][model_index]\n",
        "    new_q = current_q + q_learner.learning_rate * (terminal_reward - current_q)\n",
        "    q_learner.q_table[last_state][model_index] = new_q\n",
        "\n",
        "    # Print training statistics\n",
        "    print(f\"Training complete!\")\n",
        "    print(f\"Final epsilon: {q_learner.epsilon:.4f}\")\n",
        "    print(f\"Cheap model uses: {q_learner.stats['cheap_model_uses']}\")\n",
        "    print(f\"Expensive model uses: {q_learner.stats['expensive_model_uses']}\")\n",
        "    print(f\"Average reward: {np.mean(q_learner.stats['rewards']):.4f}\")\n",
        "\n",
        "    # Test model\n",
        "    test_dataset = gsm8k_dataset['test']\n",
        "    correct_predictions = 0\n",
        "    total_predictions = len(test_dataset)\n",
        "    for i in tqdm(range(1), desc=\"Testing Q-learner\"):\n",
        "        test_problem = test_dataset[i]\n",
        "        test_state = q_learner.get_state(test_problem[\"question\"])\n",
        "        model_index, model = q_learner.choose_model(test_state)\n",
        "\n",
        "        # Process the test problem\n",
        "        model_output = process_problem(test_problem, model_index, models)\n",
        "\n",
        "        predicted_answer = extract_answer(model_output)\n",
        "        true_answer = extract_answer(test_problem[\"answer\"])\n",
        "        print(f\"Predicted: {predicted_answer}, True: {true_answer}\")\n",
        "        is_correct = (predicted_answer == true_answer) if predicted_answer and true_answer else False\n",
        "\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "khs_-ttuM1Fx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn(dataset, models):\n",
        "    dqn_learner = DQN_Learner(\n",
        "        models,\n",
        "        learning_rate=0.001,\n",
        "        discount_factor=0.9,\n",
        "        epsilon=0.1,\n",
        "        epsilon_decay=0.995,\n",
        "        epsilon_min=0.01\n",
        "    )\n",
        "\n",
        "    dataset = gsm8k_dataset['train']\n",
        "    for i in tqdm(range(5), desc=\"Training DQN learner\"):\n",
        "        current_problem = dataset[i]\n",
        "        next_problem = dataset[i+1]\n",
        "\n",
        "        # Get current state\n",
        "        current_state = dqn_learner.get_state(current_problem[\"question\"])\n",
        "\n",
        "        # Choose model using the DQN policy\n",
        "        model_index, model = dqn_learner.choose_model(current_state)\n",
        "\n",
        "        # Process the current problem\n",
        "        model_output = process_problem(current_problem, model_index, models)\n",
        "\n",
        "        print(f\"\\nProblem {i}: {current_problem['question']}\")\n",
        "        print(f\"\\nChosen model: {model_index} ({'cheap' if model_index == 0 else 'expensive'})\")\n",
        "        print(f\"\\nModel output: {model_output}\")\n",
        "\n",
        "        # Extract answers and check correctness\n",
        "        predicted_answer = extract_answer(model_output)\n",
        "        print(f\"Predicted answer: {predicted_answer}\")\n",
        "        true_answer = extract_answer(current_problem[\"answer\"])\n",
        "        print(f\"True answer: {true_answer}\")\n",
        "        is_correct = (predicted_answer == true_answer) if predicted_answer and true_answer else False\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = dqn_learner.calculate_reward(model_index, is_correct)\n",
        "\n",
        "        # Get next state\n",
        "        next_state = dqn_learner.get_state(next_problem[\"question\"])\n",
        "\n",
        "        # Train the DQN model\n",
        "        dqn_learner.train(current_state, model_index, reward, next_state, done=False)\n",
        "\n",
        "        # Decay epsilon after each problem\n",
        "        dqn_learner.decay_epsilon()\n",
        "\n",
        "    # Save the trained DQN model\n",
        "    # dqn_learner.save_model('dqn_model.pth')\n",
        "\n",
        "    # Evaluation phase (optional)\n",
        "    print(\"\\nEvaluation Phase:\")\n",
        "    test_dataset = gsm8k_dataset['test']\n",
        "    correct_predictions = 0\n",
        "    total_predictions = min(10, len(test_dataset))  # Evaluate on first 10 test problems\n",
        "\n",
        "    for i in range(total_predictions):\n",
        "        problem = test_dataset[i]\n",
        "        state = dqn_learner.get_state(problem[\"question\"])\n",
        "\n",
        "        # Use the trained policy with no exploration (epsilon=0)\n",
        "        epsilon_backup = dqn_learner.epsilon\n",
        "        dqn_learner.epsilon = 0\n",
        "        model_index, _ = dqn_learner.choose_model(state)\n",
        "        dqn_learner.epsilon = epsilon_backup\n",
        "\n",
        "        model_output = process_problem(problem, model_index, models)\n",
        "        predicted_answer = extract_answer(model_output)\n",
        "        true_answer = extract_answer(problem[\"answer\"])\n",
        "\n",
        "        if predicted_answer == true_answer:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        print(f\"Test Problem {i}: {'Correct' if predicted_answer == true_answer else 'Incorrect'}\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_predictions/total_predictions:.2f}\")\n",
        "    print(f\"Model usage statistics: Cheap model: {dqn_learner.stats['cheap_model_uses']}, Expensive model: {dqn_learner.stats['expensive_model_uses']}\")"
      ],
      "metadata": {
        "id": "3s381AKOM3Df"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ppo(dataset, models, num_episodes=5):\n",
        "    \"\"\"Train a PPO agent to select models based on problem characteristics\"\"\"\n",
        "    ppo_agent = PPO_Agent(models)\n",
        "\n",
        "    train_data = dataset['train']\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"Episode {episode+1}/{num_episodes}\")\n",
        "\n",
        "        # Process multiple problems in each episode\n",
        "        for i in range(min(10, len(train_data) - 1)):\n",
        "            current_problem = train_data[i]\n",
        "            next_problem = train_data[i+1]\n",
        "\n",
        "            # Get current state\n",
        "            current_state = ppo_agent.get_state(current_problem[\"question\"])\n",
        "\n",
        "            # Choose model using policy network\n",
        "            model_index, model, action_prob = ppo_agent.choose_model(current_state)\n",
        "\n",
        "            # Process the problem using the chosen model\n",
        "            model_output = process_problem(current_problem, model_index, models)\n",
        "\n",
        "            print(f\"\\nProblem {i}: {current_problem['question']}\")\n",
        "            print(f\"\\nChosen model: {model_index} ({'cheap' if model_index == 0 else 'expensive'})\")\n",
        "            print(f\"\\nModel output: {model_output}\")\n",
        "\n",
        "            # Check correctness\n",
        "            predicted_answer = extract_answer(model_output)\n",
        "            print(f\"Predicted answer: {predicted_answer}\")\n",
        "            true_answer = extract_answer(current_problem[\"answer\"])\n",
        "            print(f\"True answer: {true_answer}\")\n",
        "            is_correct = (predicted_answer == true_answer) if predicted_answer and true_answer else False\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = ppo_agent.calculate_reward(model_index, is_correct)\n",
        "\n",
        "            # Get next state\n",
        "            next_state = ppo_agent.get_state(next_problem[\"question\"])\n",
        "\n",
        "            # Store experience\n",
        "            done = (i == min(10, len(train_data) - 1) - 1)\n",
        "            ppo_agent.remember(current_state, model_index, reward, next_state, action_prob, done)\n",
        "\n",
        "        # Update policy after collecting experiences from this episode\n",
        "        ppo_agent.update_policy()\n",
        "\n",
        "        # Evaluate periodically\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            evaluate_ppo(ppo_agent, dataset['test'][:10])\n",
        "\n",
        "    # Save the trained model\n",
        "    ppo_agent.save_model('ppo_policy.pt', 'ppo_value.pt')\n",
        "\n",
        "    return ppo_agent"
      ],
      "metadata": {
        "id": "bd7mAXPPQL9g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_dataset = {\n",
        "    'train': get_dataset()[0],\n",
        "    'test': get_dataset()[1]\n",
        "}\n",
        "\n",
        "models = [get_model('phi2'), get_model('wizardmath')]\n",
        "\n",
        "run_q_learner(gsm8k_dataset, models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "493cd09ea4574d199a670b177dbd3a28",
            "eab44f4955e24af982adadd52a578d21",
            "c7da4a57d3634e7f85a4f49e6a38efd9",
            "45eea6b803a54fac8f2a8a2eab809310",
            "b46f41474023400186983a6b865eaac4",
            "d33a558a2c20491abe28bd45616fa16f",
            "3691b448052d486d89fd0b12aa76d9c4",
            "e5ce879526ea434dacb4513f57d06f48",
            "d5de0421fb8f439b8e157bb3667aa81f",
            "ff5d646401544ee09f557c9cba168074",
            "a4a6bef1a0d7487b8b594dd06aa85cf6",
            "0baf9ba3ddb8495f916e49e38c6628ec",
            "14d2c5b05be6445289018e982d35e41f",
            "b092edf5c3a64c83a831d3a6035d5a14",
            "d7b6207ef8f844fb8038f400429a04c3",
            "f0947379bc8d427e8d6fcf50d6302d9f",
            "c937b0603f744370b44acc474ef26264",
            "31e0984faf91453493f9443122ec838f",
            "1f520526dc9d4ccbb2be8259d69f344b",
            "bdf8c5b8140b4f7ca5de7573b0082ea6",
            "30bc9c7b1e98402eada4c52113916e78",
            "d08ac3d93918468481509d81a0527e59"
          ]
        },
        "id": "VFb6jkbmgkL5",
        "outputId": "bdd98299-ea0c-42e0-e4fc-7b2a22d45ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "493cd09ea4574d199a670b177dbd3a28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0baf9ba3ddb8495f916e49e38c6628ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Q-learner:   0%|          | 0/7473 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Training Q-learner:   0%|          | 1/7473 [00:53<110:42:40, 53.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 72\n",
            "True answer: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 2/7473 [01:45<109:28:07, 52.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: None\n",
            "True answer: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 3/7473 [02:37<108:59:58, 52.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 5\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 4/7473 [03:30<109:13:06, 52.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 42\n",
            "True answer: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 5/7473 [03:41<77:45:23, 37.48s/it] Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 624\n",
            "True answer: 624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 6/7473 [04:33<88:08:18, 42.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 35\n",
            "True answer: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 7/7473 [04:42<65:27:48, 31.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 48\n",
            "True answer: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 8/7473 [04:50<50:02:14, 24.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 16\n",
            "True answer: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 9/7473 [05:44<68:52:54, 33.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 45\n",
            "True answer: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 10/7473 [06:36<81:00:43, 39.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: None\n",
            "True answer: 990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 11/7473 [06:52<66:25:57, 32.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 119\n",
            "True answer: 121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 12/7473 [07:45<79:39:25, 38.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 10\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 13/7473 [07:54<61:21:53, 29.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 85\n",
            "True answer: 85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 14/7473 [08:47<75:34:50, 36.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 30\n",
            "True answer: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 15/7473 [09:39<85:25:21, 41.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 0\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 16/7473 [10:05<76:18:00, 36.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 20\n",
            "True answer: 448000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 17/7473 [10:58<86:03:43, 41.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 514\n",
            "True answer: 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 18/7473 [11:50<92:37:08, 44.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 19\n",
            "True answer: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 19/7473 [12:09<76:35:10, 36.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 16\n",
            "True answer: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 20/7473 [13:02<86:44:54, 41.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 16\n",
            "True answer: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 21/7473 [13:55<93:31:42, 45.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 38\n",
            "True answer: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 22/7473 [14:11<75:11:36, 36.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 1080\n",
            "True answer: 1080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 23/7473 [15:04<85:39:25, 41.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 1\n",
            "True answer: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 24/7473 [15:19<69:05:21, 33.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 5\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 25/7473 [15:28<53:58:07, 26.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 62\n",
            "True answer: 62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 26/7473 [16:23<71:47:18, 34.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 110\n",
            "True answer: 110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 27/7473 [16:32<55:55:16, 27.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 400\n",
            "True answer: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 28/7473 [16:48<49:15:23, 23.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 360\n",
            "True answer: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 29/7473 [16:59<41:04:35, 19.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 8\n",
            "True answer: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 30/7473 [17:17<40:01:32, 19.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 1000\n",
            "True answer: 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 31/7473 [17:29<35:36:42, 17.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 6\n",
            "True answer: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 32/7473 [18:22<57:28:06, 27.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 1200\n",
            "True answer: 1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 33/7473 [19:16<73:37:17, 35.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 10\n",
            "True answer: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 34/7473 [19:24<56:30:42, 27.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 34\n",
            "True answer: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 35/7473 [19:31<44:07:15, 21.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 5250\n",
            "True answer: 5250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 36/7473 [19:42<37:38:31, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 36\n",
            "True answer: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   0%|          | 37/7473 [19:50<31:17:16, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 15\n",
            "True answer: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 38/7473 [20:13<36:04:10, 17.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 5\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 39/7473 [21:06<58:18:05, 28.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 72\n",
            "True answer: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 40/7473 [21:21<49:56:02, 24.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 15\n",
            "True answer: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 41/7473 [21:36<44:03:34, 21.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 476\n",
            "True answer: 476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 42/7473 [22:28<63:04:44, 30.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 500\n",
            "True answer: 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 43/7473 [22:46<55:38:22, 26.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 99\n",
            "True answer: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 44/7473 [22:55<44:11:55, 21.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 60\n",
            "True answer: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 45/7473 [23:14<43:01:05, 20.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 300\n",
            "True answer: 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 46/7473 [24:07<62:32:02, 30.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 99\n",
            "True answer: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 47/7473 [24:16<49:53:01, 24.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 1920\n",
            "True answer: 1920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 48/7473 [24:37<47:54:58, 23.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 15\n",
            "True answer: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 49/7473 [24:46<38:38:48, 18.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 10\n",
            "True answer: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 50/7473 [24:52<30:48:29, 14.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 48\n",
            "True answer: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 51/7473 [25:44<53:47:49, 26.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 100\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 52/7473 [26:04<50:15:04, 24.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 160\n",
            "True answer: 160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 53/7473 [26:21<45:24:36, 22.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 5\n",
            "True answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 54/7473 [26:33<39:04:41, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 36\n",
            "True answer: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 55/7473 [27:24<59:15:18, 28.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 13\n",
            "True answer: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 56/7473 [27:37<49:11:47, 23.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 75\n",
            "True answer: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 57/7473 [28:28<66:14:38, 32.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 0 (cheap)\n",
            "Predicted answer: 31\n",
            "True answer: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Q-learner:   1%|          | 58/7473 [28:41<54:06:11, 26.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chosen model: 1 (expensive)\n",
            "Predicted answer: 2\n",
            "True answer: 2\n"
          ]
        }
      ]
    }
  ]
}